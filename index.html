<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.12.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>FirstModulAR: FMAR High Level Documentation</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="doxygen-awesome.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="logos.png"/></td>
  <td id="projectalign">
   <div id="projectname">FirstModulAR<span id="projectnumber">&#160;0.1.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.12.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search',true);
  $(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('index.html',''); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title"><a class="el" href="namespaceFMAR.html">FMAR</a> High Level Documentation </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="md_highLevelDocumentation"></a></p>
<p><b>FirstModulAR</b> (<a class="el" href="namespaceFMAR.html">FMAR</a>) is a framework for quickly creating AR interfaces for first-responder use cases. <a class="el" href="namespaceFMAR.html">FMAR</a> handles many cumbersome aspects of the AR development process, including cross-platform device management, interactions with 2D Unity canvases in 3D space, "mounting" of interfaces onto key points on the user's body, and more. The framework also implements many "example" interfaces which are intended to act as proofs-of-concept for many of the types of interfaces that could be used to enhance first-reponsder performance (for example, a map interface which draws the surrounding environment in relation to Points of Interest in the environment, HUD elements which display critical information such as remaining air in a SCBA tank, etc.).</p>
<h1><a class="anchor" id="autotoc_md126"></a>
Quick Start</h1>
<h2><a class="anchor" id="autotoc_md127"></a>
Using FMAR Packages with Unity Package Manager</h2>
<p><a class="el" href="namespaceFMAR.html">FMAR</a> packages are hosted on a custom Unity Package Manager (UPM) registry, allowing you to quickly integrate them into your projects. Follow these steps to add and access the <a class="el" href="namespaceFMAR.html">FMAR</a> packages from <code>upm.firstmodular.nextgeninteractions.com</code>.</p>
<h3><a class="anchor" id="autotoc_md128"></a>
Adding the Custom UPM Registry</h3>
<ol type="1">
<li>Open your Unity project.</li>
<li>In the top menu, navigate to <b>Edit &gt; Project Settings</b>.</li>
<li>In the <b>Project Settings</b> window, scroll down to <b>Package Manager</b>.</li>
<li>Under the <b>Scoped Registries</b> section, click the <b>+</b> button to add a new registry.</li>
<li>Fill out the registry information as follows:<ul>
<li><b>Name</b>: <code>FirstModulAR</code></li>
<li><b>URL</b>: <code><a href="https://upm.firstmodular.nextgeninteractions.com/">https://upm.firstmodular.nextgeninteractions.com/</a></code></li>
<li><b>Scopes</b>: <code>com.firstmodular</code></li>
</ul>
</li>
<li>Click "Apply".</li>
</ol>
<h3><a class="anchor" id="autotoc_md129"></a>
Accessing FMAR Packages</h3>
<p>Once the custom registry is set up, you can access all available <a class="el" href="namespaceFMAR.html">FMAR</a> packages from the <b>My Registries</b> section in Unityâ€™s Package Manager. Follow these steps:</p>
<ol type="1">
<li>Open the <b>Package Manager</b> window by going to <b>Window &gt; Package Manager</b>.</li>
<li>In the top left of the <b>Package Manager</b>, click the drop-down menu that says <b>Unity Registry</b> and select <b>My Registries</b>.</li>
<li>You will see a list of all <a class="el" href="namespaceFMAR.html">FMAR</a> packages available from the <code>upm.firstmodular.nextgeninteractions.com</code> registry.</li>
<li>Select the desired package and click <b>Install</b>.</li>
</ol>
<h3><a class="anchor" id="autotoc_md130"></a>
Additional Information</h3>
<p>For more details on using the Unity Package Manager, visit the official Unity documentation: <a href="https://docs.unity3d.com/Manual/upm-ui.html">Unity Package Manager Documentation</a>.</p>
<h2><a class="anchor" id="autotoc_md131"></a>
Setup</h2>
<p>Instructions on how to quickly create an XR-ready scene with a simple placeholder cube, using the Oculus/Meta Quest hardware environment.</p>
<ol type="1">
<li>Install packages: <code><a class="el" href="namespaceFMAR.html">FMAR</a> Core</code>, <code>Mock XR Provider</code>, <code>UnityXR XR Provider</code>, <code>Oculus XR Provider</code>, <code><a class="el" href="namespaceFMAR.html">FMAR</a> Build Window</code>.</li>
<li>Create a new scene. Delete the existing camera object.</li>
<li>Drag the <code><a class="el" href="namespaceFMAR.html">FMAR</a> Core</code> prefab (located in the <code><a class="el" href="namespaceFMAR.html">FMAR</a> Core</code> <code>Prefabs</code> folder) into the scene.</li>
<li>For each XR Provider package (Mock, UnityXR, and Oculus), navigate to the <code>Prefabs</code> folder and drag the corresponding XR Provider prefab into the scene, as a child of the <code><a class="el" href="namespaceFMAR.html">FMAR</a> Core</code> game object.</li>
<li>Click the <code><a class="el" href="namespaceFMAR.html">FMAR</a> Core</code> game object and in the Inspector, find the <code>Fmar Device Manager</code> component and click the <code>Refresh XR Providers</code> button. Each XR Provider should appear in the component's <code>XR Providers</code> list.</li>
<li>From the <code><a class="el" href="namespaceFMAR.html">FMAR</a> Core</code> <code>Prefabs</code> folder, drag the <code><a class="el" href="namespaceFMAR.html">FMAR</a> Core Rig</code> into the scene.</li>
<li>Add a cube primitive to the scene at the world origin.</li>
</ol>
<h2><a class="anchor" id="autotoc_md132"></a>
Building the Application</h2>
<p>Once the scene is setup, you may build and deploy the application.</p>
<ol type="1">
<li>Open up the <code><a class="el" href="namespaceFMAR.html">FMAR</a> Build Window</code> window by navigating through the topbar: <code>Window &gt; <a class="el" href="namespaceFMAR.html">FMAR</a> &gt; Build Window</code>.</li>
<li>In the Build Window, change the target device to Meta Quest and configure any settings as needed, as shown by the Build Window.</li>
<li>Follow the instructions in the <code><a class="el" href="namespaceFMAR.html">FMAR</a> Build Window</code> package readme file to create a new keystore profile as needed.</li>
<li>Build, and deploy the built .apk file to the headset using the <a href="https://developer.oculus.com/meta-quest-developer-hub/">Meta Quest Developer Hub</a> application.</li>
</ol>
<h1><a class="anchor" id="autotoc_md133"></a>
Architecture Summary</h1>
<p><img src="architectureDiagram.png" alt="image" class="inline"/></p>
<p>FMAR Core provides the Device Manager, which consolidates disparate input from <code>XR Providers</code> into common FMAR-universal input structures. The Core Rig further consolidates all known devices into a "rig" which represents the player's body through three data structs &ndash; a headset, a left hand/controller, and a right hand/controller. Gestures can be configured to watch the user's hands for certain poses/motions which can trigger custom logic.</p>
<p>Simple Visualization Layers can be added to the scene which draw additional information onto the world or directly mounted to the user's cone of vision, using only requiring the headset position as provided by the <code>Core Rig</code>.</p>
<p>Mount Points are intended to provide convenient locations upon which UI elements (e.g. the TabbedDisplay) can be "mounted". They provide an additional layer of nuance beyond the three simple positions provided by the <code>Core Rig</code>, providing, for example, transform positions for the user's approximated "chest" position and approximated positions of their forearms, and midpoints between various other points. Mount Points also provide built-in lerping to add an additional illusion of weight and momentum to mounted UI elements.</p>
<p>The Canvas Interactions object provides a system for interacting with 2D Unity canvas elements in XR, enabling two distinct but freely inter-usable schemes for interacting with 2D canvases &ndash; "touch screen" interactions and "raycasted" interactions.</p>
<p>CustomComponents (named as such because they are custom implementations of UI components) are custom implementations of <code>IPointerHandler</code>-inheriting UI components like the <code>Button</code> which are custom-made to behave in a way that is more suitable to XR, as compared to the default Unity UI button.</p>
<p>The TabbedDisplay implements <code>CustomComponents</code> heavily and allows the user to click on tabs to switch between multiple <code>Widgets</code>. Widgets are distinct interfaces that each perform a job. Examples of widgets include the <code>Map</code> widget, the <code>Notifications</code> widget for viewing, inspecting, and dismissing notifications, etc.</p>
<p>The RadialMenu provides a gestural way to quickly navigate through complex option trees in XR.</p>
<p>Internal packages manage and represent various aspects of the game state, but are not interacted with directly, instead providing public methods so that other packages can provide interfaces to change and view internal information.</p>
<p>In the following sections, more detail will be provided on each of these items.</p>
<h1><a class="anchor" id="autotoc_md134"></a>
Components</h1>
<h2><a class="anchor" id="autotoc_md135"></a>
Internal (Backend)</h2>
<p><a class="el" href="namespaceFMAR.html">FMAR</a> provides many packages which are designed to manage various aspects of the "game state", e.g. what are points of interest that the user should be aware of? What is the vitals status of each of the connected users? Is the user currently performing a procedure and if so, what step? These packages are considered "internal" because they do not handle input from the user directly, instead they expose public methods which other packages (e.g. UI interfaces such as buttons, <code>Gesture</code> listeners which execute UnityEvents, and <code>RadialMenu</code> link triggers) can call as needed.</p>
<p>Since they do not reference input directly, many "internal"-facing packages have fewer dependencies on the many of the packages described in the <a class="el" href="namespaceFMAR.html">FMAR</a> "input stack" described later in this document. For this reason they are being discussed before these other packages.</p>
<p>Many internal packages are also designed to replicate their information across network sessions for many users using the Unity <code>Netcode for GameObjects</code> networking framework. In the event that a developer wishes to modify one of these packages or create their own Internal package which utilizes Netcode, they should take care to understand the various challenges of multiplayer development.</p>
<h2><a class="anchor" id="autotoc_md136"></a>
FMAR Core and the Device Manager</h2>
<p><a class="el" href="namespaceFMAR.html">FMAR</a> was developed with compatbility and interoperability in mind. True universal cross-platform support is not strictly possible at this time, since many experimental features (e.g. edge detection visualization) are only available on their respective platforms (i.e. Meta/Oculus), and even in cases where established common standards do exist (e.g. OpenXR, UnityXR) not all platforms elect to use them for all features. For example, Meta exposes their normal controllers through the UnityXR layer, but does not expose their tracked hands through the UnityXR layer, despite the fact that UnityXR provides data structures that are intended to represent hands. Surely adequate reasons for these decisions are known by the respective hardware manufactures, however it does pose a difficulty for our purposes in creating a platform that transcends hardware barriers and is as interoperable as possible.</p>
<p>The solution we have created in response to these challenges is the <a class="el" href="namespaceFMAR.html">FMAR</a> XR Providers system and the <a class="el" href="namespaceFMAR.html">FMAR</a> Device Manager. Simply put, XR Providers expose all the input provided by their respective hardware platforms and convert it into universal middle-layer data structures that are recognized by all <a class="el" href="namespaceFMAR.html">FMAR</a> objects and interactions.</p>
<p>Note that with the exception of the hardware platform's respective XR Provider, <a class="el" href="namespaceFMAR.html">FMAR</a> should not (as much as possible) refer to platform-specific input data structures directly. Rather, every attempt should be made to refer to universal/common data types implemented by the <a class="el" href="namespaceFMAR.html">FMAR</a> Core package (e.g. <code>Controller</code>, <code>CommonHand</code>, etc.).</p>
<h3><a class="anchor" id="autotoc_md137"></a>
Controllers</h3>
<p><code>Controllers</code> are the basis for the majority of "input" in <a class="el" href="namespaceFMAR.html">FMAR</a>.</p>
<p>By this definition, a controller is any input "device" which can expose a position, a pointing direction, and a "pressed" boolean. This could either be a physical controller with buttons and triggers, or even a tracked hand, since a tracked hand can provide all of the necessary information just discussed.</p>
<h3><a class="anchor" id="autotoc_md138"></a>
CommonHands</h3>
<p>Any tracked hand exposed by an XR Provider is represented internally as an <a class="el" href="namespaceFMAR.html">FMAR</a> <code>CommonHand</code>.</p>
<p>CommonHands have the property of being considered, for all intents and purposes, <em>Controller</em> objects in their own right. Making the pinch gesture with the hand is equivalent to triggering the <code>Pressed</code> event for a controller.</p>
<p>However, unlike normal controllers, CommonHands have the unique ability to trigger <code>Gestures</code>.</p>
<h2><a class="anchor" id="autotoc_md139"></a>
Gestures</h2>
<p><code>Gestures</code> are optional and not actually a feature of <code><a class="el" href="namespaceFMAR.html">FMAR</a> Core</code>. However, they do add an additional layer of input parsing that can provide some useful shorthand inputs for users.</p>
<p>Gestures can be "tracked" by adding Gesture components to the scene which are configured to "listen" for specific gestures. For example, you could add an <code>Open Palm Thrust Gesture</code> to the scene, and configure it to trigger when the user holds their left hand open with their palm towards the sky, and then thrusts their hand upwards. You can then set <code>UnityEvents</code> from other objects in the scene to occur when the gesture triggers.</p>
<p>The example described above is how the <code>TabbedDisplay</code> is summoned in the various <a class="el" href="namespaceFMAR.html">FMAR</a> demo scenes. To dismiss the <code>TabbedDisplay</code>, an additional <code>Open Palm Thrust Gesture</code> is added, simply configured to listen for open palm thrusts in the opposite (downwards) direction.</p>
<p>Currently supported gestures are:</p><ul>
<li><code>Open Palm Thrust Gesture</code> (already noted)</li>
<li><code>Directional Pinch Gesture</code></li>
<li><code>Pinch And Move Gesture</code></li>
<li><code>Thumbs Up Gesture</code></li>
</ul>
<h2><a class="anchor" id="autotoc_md140"></a>
The Core Rig</h2>
<p>The Core Rig is provided as a way to take the potentially numerous, abstract "devices" exposed by the <em>DeviceManager</em>, and using them, more intuitively represent the user as a humanoid structure composed of three points: two controllers (left-handed and right-handed) and a headset.</p>
<p>Most other packages also reference the Core Rig to access positional/rotational data of the controllers/headset, since in most cases there is no need to (for example) process the position of a controller that is not being actively used.</p>
<p>A camera is also provided by default on the Core Rig root GameObject. The position and rotation of Core Rig root GameObject (and thus the camera) are constrained to the position and rotation of the headset struct exposed by the Device Manager.</p>
<h2><a class="anchor" id="autotoc_md141"></a>
Visualization Layers</h2>
<p>Certain packages provide simple visualizations over the game world, either as 3D spatialized glyphs in the world that are drawn positionally in proximity to the real-world objects they are supposed to mark (e.g. in the case of the <code>Indicators</code> package), or as an overlay that is kept continuously in the user's cone of vision (e.g. in the case of the <code>PeripheryFader</code> package). These packages are fairly innocuous in that they require very little "input". In most cases the only input they require is the position of the headset as exposed by the Core Rig, so that they can, for example, always rotate <code>Indicators</code> to face the camera head-on, or keep the <code>PeripheryFader</code> mounted to the user's head.</p>
<h2><a class="anchor" id="autotoc_md142"></a>
Mount Points</h2>
<p>Over the course of XR development, very often the need arises for behavior where a UI element (for example, the soon-to-be-discussed <code>TabbedDisplay</code>) should be "attached" to a point on the user's <code>Core Rig</code> &ndash; either the headset (creating a sort of HUD effect) or on either one of the hands.</p>
<p>However, additional features were desired that added layers of complexity, that for purposes of simplicity were not appropriate to add directly to the <code>Core Rig</code>.</p>
<p>For example, in many cases if a UI element configured to follow a point on the Core Rig, does so in such a way that it is "perfectly" constrained to a point on the Core Rig (i.e. on a frame-by-frame basis, perfectly and instantly setting its position and rotation to be that of the point exposed by the Core Rig), the overall effect is interestingly uncanny. The UI element is decidedly virtual, seemingly having no weight or intertia whatsoever. We have found that having a slight lerping/smooth damping effect associated with UI elements that are bound in this way gives them the illusion of physical weight and momentum, and counterintuitively enough, makes them feel even more responsive and tight to the user.</p>
<p>The Mount Point system is a way of providing these desired features in a consolidated way.</p>
<h2><a class="anchor" id="autotoc_md143"></a>
Canvas Interactions</h2>
<p>The <code>Canvas Interactions</code> object provides a middle-layer which enables users to interact with default Unity canvas systems using a custom-made Unity input module, the <code>XrInputModule</code>.</p>
<p>There are two primary methods of interacting with 2D canvases in 3D space:</p><ul>
<li>"Touch-screen" interactions where the user holds their hand close to the canvas being interacted on, and the user's fingertip is considered as a cursor. The cursor "clicks" a canvas element when it passes through the plane created by the canvas.</li>
<li>"Raycast" interactions where the user's hand or controller is held distantly/remotely from the canvas being interacted on. A raycasted line is shot from the user's hand/controller, creating a cursor on the canvas that is equivalent to a mouse cursor. The cursor "clicks" when either the controller's trigger is pulled (if the user is using a physical controller) or when the user makes the pinch gesture (if the user is using their hand as a controller).</li>
</ul>
<p>Either method can be used at any time, the system is designed to allow freely switching between the two methods.</p>
<p>The touch screen method works something like this:</p><ul>
<li>The position of the fingertip is checked to see if it is within a certain range of any "touchable" surfaces e.g. the <code>TabbedDisplay</code>.</li>
<li>If it is, a point is drawn from the fingertip to the closest point on the plane, marking that point in 3D space.</li>
<li>The 3D point is then converted into a 2D "screen" point, effectively emulating a mouse cursor and enabling the usage of default Unity canvas items such as buttons and scrollbars, or for that matter any <code>IPointerHandler</code>-inheriting components.</li>
<li>If the fingertip passes through the touchable surface, the proper <code>EventSystem</code> events are triggered e.g. <code>OnPointerDown</code>.</li>
</ul>
<p>The raycast method works something like this:</p><ul>
<li>The Canvas Interactions system continuously raycasts a line from the controller/hand in the pointing direction of the controller/hand, reacting and creating a cursor when the raycast hits specially-marked "raycastable" surfaces e.g. the <code>TabbedDisplay</code>.</li>
<li>The raycasted collision point is then marked in 3D space.</li>
<li>The 3D point is then converted into a 2D "screen" point, effectively emulating a mouse cursor and enabling the usage of default Unity canvas items such as buttons and scrollbars, or for that matter any <code>IPointerHandler</code>-inheriting components.</li>
<li>If a trigger press/pinch gesture is detected, the proper <code>EventSystem</code> events are triggered e.g. <code>OnPointerDown</code>.</li>
</ul>
<h2><a class="anchor" id="autotoc_md144"></a>
FMAR UI CustomComponents</h2>
<p>As described above, the <code>Canvas Interactions</code> system is designed to work with default Unity canvas items such as <code>UnityEngine.UI.Button</code> and <code>UnityEngine.UI.Scrollbar</code>. These <code>IPointerHandler</code>-inheriting components are quite useful as an out-of-the-box solutions and for most use cases are typically sufficient for representing 2D UI elements such as buttons.</p>
<p>However, these components' behaviors are predefined in ways that are not always optimal for an XR interface. For instance, the default <code>Button</code> behavior is to trigger its <code>onClick</code> event when both the <code>IPointerDown</code> and <code>IPointerUp</code> events have been triggered in succession on the same component. In a traditional 2D desktop environment, this would result in behavior where the user doesn't really "click" a button until they both click down and let up the mouse cursor, providing a bit more insurance for any given button click that the event is the result of an intentional input made by the user, and not an accidental misclick.</p>
<p>However, this generally-sound decision simply becomes a nuisance in certain XR scenarios. For instance, using the "touch-screen" interaction paradigm described in the <code>Canvas Interactions</code> section, this down-and-up behavior is much more difficult to consistently execute when the touch screen in question is a virtual abstraction with no physical surface to provide guidance and reference for the touch action &ndash; as opposed to a real physical touch screen which would provide immediate touch sensory information to the user when the fingertip touches the surface of the screen. In light of these differences, the "virtual" touch screen feels markedly more usable when click events trigger as soon as the <code>IPointerDown</code> event is triggered.</p>
<p>To this end, the <code>CustomComponents</code> button was created to serve as a replacement of the default Unity button. These custom buttons behave in the way described above (immediately triggering click event on pointer down). Additionally, several convenience features have been added to the button such as visual mouseover effects, sound effects on button press, etc.</p>
<p>In almost every widget, 2D interface, etc. the <code>CustomComponents</code> button has been used in place of the default Unity button. If any exceptions to this rule are found, such cases are likely unintentional oversights.</p>
<h2><a class="anchor" id="autotoc_md145"></a>
Widgets and the TabbedDisplay</h2>
<p>The <code>TabbedDisplay</code> provides an AR interface that should be instantly familiar to any denizen of the modern age &ndash; a flat surface which is equivalent to the touch screen of a tablet or smartphone. While this interface, from a theoretical standpoint, does not strictly take advantage of all of the rich, gestural input that XR tracking can provide, its redeeming virtue is that it is instantly recognizable to the majority of users likely to be using <a class="el" href="namespaceFMAR.html">FMAR</a>, ensuring that the majority of users will be able, at least in some way, access the majority of <a class="el" href="namespaceFMAR.html">FMAR</a> features.</p>
<p>The TabbedDisplay as a whole is attached, at an offset, to the user's left wrist, allowing users to easily use their right hand to interact with the elements on the display surface. This decision was made with the fact that most users will be right-handed in mind &ndash; but a bit of refactoring would allow users to change their 'dominant' hand for TabbedDisplay usage. However, even left handed users can still interact with the display easily. The display follows the user's left hand when the palm is facing upright, but will remain in place when the user's hand is rotated with the palm facing downwards, allowing other interactions to be performed.</p>
<p>The TabbedDisplay is composed of numerous "tabs", each of which includes a <code>Widget</code>, a distinct 2D interface which is usually bound to a certain specific backend system (or in some cases several backend systems).</p>
<p>Any combination of Widgets can be added as desired into the TabbedDisplay of a given <a class="el" href="namespaceFMAR.html">FMAR</a> scene.</p>
<p>Some examples of Widgets are:</p><ul>
<li><code>Map</code> (shows <code>Points of Interest</code> in proximity to the user, allows zooming in and out)</li>
<li><code>Points of Interest</code> (allows the user to view and edit a list of all known Points of Interest)</li>
<li><code>Notifications</code> (allows the user to view, inspect, and dismiss notifications)</li>
<li><code>Procedures</code> (allows the user to select, progress through, and navigate procedures)</li>
</ul>
<h2><a class="anchor" id="autotoc_md146"></a>
The Radial Menu</h2>
<p>The <code>Radial Menu</code> is a method of UI traversal that takes advantage of the unique affordances of XR input schemes, providing users with the ability to quickly navigate vast hierarchies of options using broad, gestural input.</p>
<p>However, the drawback is that, unlike in the case of the <code>Tabbed Display</code>, most users will have no experience with menus of this nature and therefore will not understand its usage intuitively. The Radial Menu, then, can be considered as a powerful tool that is locked behind a minor usability barrier. That said, the Radial Menu is simple enough that instructing a new user in its usage should not have to take more than a couple of minutes, and some technically-inclined users should even be able to figure it out through a bit of self-guided experimentation.</p>
<p>Currently the Radial Menu is accessed through a double-pinch gesture made on the right hand. Once the user does this and "holds" the second pinch, a ring will appear around the user's hand for as long as the pinch is sustained. The ring is divided into distinct "quadrants", each of which represents a selectable option. The user can select an option by moving their hand out of the center of the ring into any of the quadrants as desired. Doing so will highlight the selected quadrant, giving the user visual feedback about their current selection. To confirm the current selection, the user simply releases the pinch.</p>
<p>In addition to this, an option can possess a "nested" options set which then creates another ring menu with its own distinct options set. Options that possess a nested set are indicated with an arrow/caret pointing in the same direction as the pointing direction of the quadrant. To navigate into such a nested set, the user simply moves their hand all the way out of the ring in the direction of the desired option.</p>
<h2><a class="anchor" id="autotoc_md147"></a>
Links</h2>
<p><code>Links</code> are a somewhat unconventional but, quite useful way to embed an action in a string. Originally they were devised as an alternative way to replicate a lambda function across the network. Their inspiration and namesake is rooted in the often very-verbose enormous web URLs that contain all sorts of data that is specific to a given user session, often containing all sorts of information as a massive base-64 string. <a class="el" href="namespaceFMAR.html">FMAR</a> links are a bit more human-readable than that, usually occurring as plain English keywords, but the name stuck as links are just simple slash-separated strings that indicate an action the system should take, or a place the user should be directed to.</p>
<p>This system, again, is somewhat unconventional and experimental. It may take a bit of time and experimentation for developers who areapproaching the project for the first time to get a sense of their use cases and the types of situation in which they are useful.</p>
<h1><a class="anchor" id="autotoc_md148"></a>
Conclusion</h1>
<p>In conclusion, <a class="el" href="namespaceFMAR.html">FMAR</a> is a framework for quickly creating first responder AR interfaces. Using the <a class="el" href="namespaceFMAR.html">FMAR</a> "input stack" (i.e. the Device Manager, Core Rig, and Canvas Interactions), developers can quickly create useful AR applications by using existing interfaces, widgets, or visualization overlays, or designing new ones as needed &ndash; without having to engineer solutions to low-level problems like cross-platform compatibility, device management, etc.</p>
<h1><a class="anchor" id="autotoc_md149"></a>
License</h1>
<p>FirstModulAR is licensed under the Apache License 2.0.</p>
<p>You are free to use, modify, and distribute the framework as long as you adhere to the terms of the Apache License, which includes appropriate attribution to <a class="el" href="namespaceFMAR.html">FMAR</a> and maintaining the license notice in any distributed version.</p>
<p>A full copy of the Apache License 2.0 can be found in the root directory of this repository under the LICENSE file, as well as within each FirstModulAR package. </p>
</div></div><!-- PageDoc -->
<a href="doxygen_crawl.html"></a>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.12.0 </li>
  </ul>
</div>
</body>
</html>
